; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt < %s -msan-check-access-address=0 -S -passes=msan 2>&1 | FileCheck  \
; RUN: %s
; RUN: opt < %s -msan-check-access-address=0 -msan-track-origins=1 -S          \
; RUN: -passes=msan 2>&1 | FileCheck -check-prefix=CHECK                       \
; RUN: -check-prefix=CHECK-ORIGINS %s
; REQUIRES: x86-registered-target

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Store intrinsic.

define void @StoreIntrinsic(ptr %p, <4 x float> %x) nounwind uwtable sanitize_memory {
; CHECK-ORIGINS-LABEL: define void @StoreIntrinsic(
; CHECK-ORIGINS-SAME: ptr [[P:%.*]], <4 x float> [[X:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-ORIGINS-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
; CHECK-ORIGINS-NEXT:    [[TMP2:%.*]] = load i32, ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_origin_tls to i64), i64 8) to ptr), align 4
; CHECK-ORIGINS-NEXT:    call void @llvm.donothing()
; CHECK-ORIGINS-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[P]] to i64
; CHECK-ORIGINS-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 87960930222080
; CHECK-ORIGINS-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
; CHECK-ORIGINS-NEXT:    [[TMP6:%.*]] = add i64 [[TMP4]], 17592186044416
; CHECK-ORIGINS-NEXT:    [[TMP7:%.*]] = and i64 [[TMP6]], -4
; CHECK-ORIGINS-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
; CHECK-ORIGINS-NEXT:    store <4 x i32> [[TMP1]], ptr [[TMP5]], align 1
; CHECK-ORIGINS-NEXT:    [[TMP9:%.*]] = bitcast <4 x i32> [[TMP1]] to i128
; CHECK-ORIGINS-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP9]], 0
; CHECK-ORIGINS-NEXT:    br i1 [[_MSCMP]], label %[[BB10:.*]], label %[[BB14:.*]], !prof [[PROF1:![0-9]+]]
; CHECK-ORIGINS:       [[BB10]]:
; CHECK-ORIGINS-NEXT:    store i32 [[TMP2]], ptr [[TMP8]], align 4
; CHECK-ORIGINS-NEXT:    [[TMP11:%.*]] = getelementptr i32, ptr [[TMP8]], i32 1
; CHECK-ORIGINS-NEXT:    store i32 [[TMP2]], ptr [[TMP11]], align 4
; CHECK-ORIGINS-NEXT:    [[TMP12:%.*]] = getelementptr i32, ptr [[TMP8]], i32 2
; CHECK-ORIGINS-NEXT:    store i32 [[TMP2]], ptr [[TMP12]], align 4
; CHECK-ORIGINS-NEXT:    [[TMP13:%.*]] = getelementptr i32, ptr [[TMP8]], i32 3
; CHECK-ORIGINS-NEXT:    store i32 [[TMP2]], ptr [[TMP13]], align 4
; CHECK-ORIGINS-NEXT:    br label %[[BB14]]
; CHECK-ORIGINS:       [[BB14]]:
; CHECK-ORIGINS-NEXT:    store <4 x float> [[X]], ptr [[P]], align 1
; CHECK-ORIGINS-NEXT:    ret void
;
  call void @llvm.x86.sse.storeu.ps(ptr %p, <4 x float> %x)
  ret void
}

declare void @llvm.x86.sse.storeu.ps(ptr, <4 x float>) nounwind



; Load intrinsic.

define <16 x i8> @LoadIntrinsic(ptr %p) nounwind uwtable sanitize_memory {
; CHECK-ORIGINS-LABEL: define <16 x i8> @LoadIntrinsic(
; CHECK-ORIGINS-SAME: ptr [[P:%.*]]) #[[ATTR0]] {
; CHECK-ORIGINS-NEXT:    call void @llvm.donothing()
; CHECK-ORIGINS-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[P]] to i64
; CHECK-ORIGINS-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 87960930222080
; CHECK-ORIGINS-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
; CHECK-ORIGINS-NEXT:    [[TMP4:%.*]] = add i64 [[TMP2]], 17592186044416
; CHECK-ORIGINS-NEXT:    [[TMP5:%.*]] = and i64 [[TMP4]], -4
; CHECK-ORIGINS-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
; CHECK-ORIGINS-NEXT:    [[_MSLD:%.*]] = load <16 x i8>, ptr [[TMP3]], align 1
; CHECK-ORIGINS-NEXT:    [[TMP7:%.*]] = load i32, ptr [[TMP6]], align 4
; CHECK-ORIGINS-NEXT:    [[CALL:%.*]] = call <16 x i8> @llvm.x86.sse3.ldu.dq(ptr [[P]])
; CHECK-ORIGINS-NEXT:    store <16 x i8> [[_MSLD]], ptr @__msan_retval_tls, align 8
; CHECK-ORIGINS-NEXT:    store i32 [[TMP7]], ptr @__msan_retval_origin_tls, align 4
; CHECK-ORIGINS-NEXT:    ret <16 x i8> [[CALL]]
;
  %call = call <16 x i8> @llvm.x86.sse3.ldu.dq(ptr %p)
  ret <16 x i8> %call
}

declare <16 x i8> @llvm.x86.sse3.ldu.dq(ptr %p) nounwind



; Simple NoMem intrinsic
; Check that shadow is OR'ed, and origin is Select'ed
; And no shadow checks!

define <8 x i16> @Pmulhuw128(<8 x i16> %a, <8 x i16> %b) nounwind uwtable sanitize_memory {
; CHECK-ORIGINS-LABEL: define <8 x i16> @Pmulhuw128(
; CHECK-ORIGINS-SAME: <8 x i16> [[A:%.*]], <8 x i16> [[B:%.*]]) #[[ATTR0]] {
; CHECK-ORIGINS-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr @__msan_param_tls, align 8
; CHECK-ORIGINS-NEXT:    [[TMP2:%.*]] = load i32, ptr @__msan_param_origin_tls, align 4
; CHECK-ORIGINS-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 16) to ptr), align 8
; CHECK-ORIGINS-NEXT:    [[TMP4:%.*]] = load i32, ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_origin_tls to i64), i64 16) to ptr), align 4
; CHECK-ORIGINS-NEXT:    call void @llvm.donothing()
; CHECK-ORIGINS-NEXT:    [[_MSPROP:%.*]] = or <8 x i16> [[TMP1]], [[TMP3]]
; CHECK-ORIGINS-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP3]] to i128
; CHECK-ORIGINS-NEXT:    [[TMP6:%.*]] = icmp ne i128 [[TMP5]], 0
; CHECK-ORIGINS-NEXT:    [[TMP7:%.*]] = select i1 [[TMP6]], i32 [[TMP4]], i32 [[TMP2]]
; CHECK-ORIGINS-NEXT:    [[CALL:%.*]] = call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> [[A]], <8 x i16> [[B]])
; CHECK-ORIGINS-NEXT:    store <8 x i16> [[_MSPROP]], ptr @__msan_retval_tls, align 8
; CHECK-ORIGINS-NEXT:    store i32 [[TMP7]], ptr @__msan_retval_origin_tls, align 4
; CHECK-ORIGINS-NEXT:    ret <8 x i16> [[CALL]]
;
  %call = call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %a, <8 x i16> %b)
  ret <8 x i16> %call
}

declare <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %a, <8 x i16> %b) nounwind

;.
; CHECK-ORIGINS: [[PROF1]] = !{!"branch_weights", i32 1, i32 1048575}
;.
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
